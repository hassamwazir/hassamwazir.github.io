---
layout: distill
title: Embodied agents using a Turtlebot3 - A toy example
date: 2024-08-28 16:00:16
description: Understanding embdied agents with a Turtlebot3 and ROS.
tags: llm
categories: post

authors:
  - name: Hassam Khan Wazir
    url: "https://hassamwazir.github.io/"
    affiliations:
      name: New York University

toc:
  sidebar: left

---

Work in progress.

<!-- This work was done in collaboration with [Liyah Coleman](https://www.linkedin.com/in/liyahcoleman/) at New York University.

## Introduction

In 2024, embodied agents have become incredibly popular. Large language models (LLMs) are now considered trustworthy enough (or so we think) to be given physical forms, and many companies are taking the next step in AI by deploying these agents in the real world. In this post, we will explore how to create an embodied agent using a [Turtlebot3 Burger](https://www.robotis.us/turtlebot-3-burger-rpi4-2gb-us/) robot (henceforth referred to as 'the robot') and ROS. Specifically, we will create a simple embodied agent that can navigate a grid. To say that using an LLM to navigate a grid is overkill would be a massive understatement, but this will serve as a good starting point for understanding how to create embodied agents.

## System overview

What we want is a simple setup where we ask the LLM to chart a course for the robot to direct it ti a goal position, given the robot's current position. The LLM will send a set of instructions to the robot that the robot can understand and execute. The robot will the execute these instructions and move to the goal position.

Since we know that the robot we are using is a differential drive robot (it can go straight and turn), we can simplify the problem by reducing the number of actions the robot can take. The robot can `step forward`, `turn left`, or `turn right`. In this experiment, we are not using any of its sensors, so the robot is essentially blind. The shorcomings and future improvements of this system will be discussed later.

## Preparing the robot

First of all, we install [ROS Humble](https://docs.ros.org/en/humble/index.html) on the Raspberry Pi 4 onboard the robot. Then we install the [Turtlebot3 ROS packages](https://emanual.robotis.com/docs/en/platform/turtlebot3/overview/) so that we can control the robot using ROS. Finally, we create a simple ROS package that will allow the robot to receive instructions from the LLM and move accordingly. There are several ways this problem can be approached, but we chose to use a simple publisher-subscriber model since this is not the focus of this project.

Once this was achieved, we were able to send an array of instructions to the robot and watch it move accordingly. For example, if the robot starts at $$ \left(0,0\right) $$ facing north, and we send the instructions `['step forward', 'turn right', 'step forward']`, the robot will move to $$ \left(1,1\right) $$ facing east. With this setup, we are basically done with the robot side of things.

## Using the LLM

Here, I will only discuss the approach that worked for us. All of the other approached we tried are discussed in the next section. We chose the [LLama-3.1-70B](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B) model for this experiment. The primary reason for this choice is that it is open source and the smallest model in the LLama family of models that worked with our prompt. We used the model as-is without any fine-tuning. To use it, we created a Hugging Face space that uses Gradio to create a simple web interface that allows us to input the robot's current position and the goal position. We only used the web interface for testing, and eventually wrote a Python script for sending and receiving messages to and from the Hugging Face space. We gave the LLM the following system prompt:

<details>
<summary>Click to see the system prompt</summary>

<p>
You are controlling a 2 DOF robot on a 50x50 grid. The robot can move one step in any of the four cardinal directions. The robot can perform the following actions:</p>

<ul style="list-style-type: none; padding: 0; margin: 0;">
  <li style="margin: 0;">- 'up': Move one unit up (increasing y coordinate by 1).</li>
  <li style="margin: 0;">- 'down': Move one unit down (decreasing y coordinate by 1).</li>
  <li style="margin: 0;">- 'left': Move one unit left (decreasing x coordinate by 1).</li>
  <li style="margin: 0;">- 'right': Move one unit right (increasing x coordinate by 1).</li>
</ul>

<p>Given a target coordinate, your task is to calculate and output the shortest sequence of commands that will move the robot from its current position to the target position.</p>

<b>Output Format:</b>
<ul style="list-style-type: none; padding: 0; margin: 0;">
  <li style="margin: 0;">- Begin with the exact phrase: 'The full list is:'.</li>
  <li style="margin: 0;">- Provide the sequence of commands as a JSON array, with each command as a string. Commands must be exactly 'up', 'down', 'left', or 'right'.</li>
  <li style="margin: 0;">- All coordinates should be formatted as JSON objects with keys 'x' and 'y' and integer values. For example, the starting position should be output as {'x': 0, 'y': 0}.</li>
  <li style="margin: 0;">- When calling tools, ensure that all arguments use this JSON object format for coordinates, with keys 'x' and 'y'.</li>
  <li style="margin: 0;">- Example of correct output: If the target coordinate is {'x': 2, 'y': 3}, your response should include: 'The full list is: ["right", "right", "up", "up", "up"]'</li>
</ul>

<p>Please ensure that all output strictly adheres to these formats. If any output is not in the correct format, redo the task and correct the output before providing the final answer.</p>

</details>

A detailed system prompt ensures that the LLM knows exactly what we want it to do. The user prompt used was much simpler:

`The robot is at 'start_position': {'x': 25, 'y': 24}. What is the shortest list of actions to take in sequence to get to 'target_position': {'x': 27, 'y': 25}?`

Sending the system and user prompts to the LLM returned the following response (Note: the robot was facing south):

`Model Response: [["The robot is at 'start_position': {'x': 25, 'y': 24}. What is the shortest list of actions to take in sequence to get to 'target_position': {'x': 27, 'y': 25}?". 'The full list is: ["right", "right", "up"].']]`

A function was written to parse the response and send the instructions to the robot. Based on the current direciton of the robot, the instructions were converted to the isntrucitons that the robot can understand (`step forward`, `turn left`, or `turn right`), and then sent to the robot. The robot is listening for instructions at all times, so it will execute the instructions sequentially as soon as they are received.

## Results, Limitations, and Future Work

The system worked perfectly for all the test cases we tried. The robot was able to move to the target position in the shortest number of steps. The LLM was able to understand the system prompt and provide the correct instructions. The robot was able to understand the instructions and move accordingly.

Now, let's discuss the limitations of this system. The robot is essentially blind, so the entire system runs in an open-loop manner. The robot only knows its initial direction (because we tell it) and can determined the final direciton after executing the intructions. The robot movements are hardcoded so a 90 -->