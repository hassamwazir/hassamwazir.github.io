<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Embodied Agents using a Turtlebot3 | Hassam Khan Wazir </title> <meta name="author" content="Hassam Khan Wazir"> <meta name="description" content="Understanding embodied agents with a Turtlebot3 and ROS."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/hassam_logo-70x70.png?06f221be7ddca9e40227d0c21ce304aa"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hassamwazir.github.io/projects/robot-llm/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Embodied Agents using a Turtlebot3",
            "description": "Understanding embodied agents with a Turtlebot3 and ROS.",
            "published": "August 28, 2024",
            "authors": [
              
              {
                "author": "Hassam Khan Wazir",
                "authorURL": "https://hassamwazir.github.io/",
                "affiliations": [
                  {
                    "name": "New York University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hassam</span> Khan Wazir </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Embodied Agents using a Turtlebot3</h1> <p>Understanding embodied agents with a Turtlebot3 and ROS.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#system-overview">System overview</a> </div> <div> <a href="#preparing-the-robot">Preparing the robot</a> </div> <div> <a href="#using-the-llm">Using the LLM</a> </div> <div> <a href="#results-limitations-and-future-work">Results, Limitations, and Future Work</a> </div> </nav> </d-contents> <p>This work was done in collaboration with <a href="https://colemanliyah.github.io/" rel="external nofollow noopener" target="_blank">Liyah Coleman</a> at the Mechatronics, Controls, and Robotics Lab at New York University Tandon School of Engineering. The code for this project can be found <a href="https://github.com/colemanliyah/social-toy-robots" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="introduction">Introduction</h2> <p>In 2024, embodied agents have become incredibly popular. Large language models (LLMs) are now considered trustworthy enough (or so we think) to be given physical forms, and many companies are taking the next step in AI by deploying these agents in the real world. In this post, we will explore how to create an embodied agent using a <a href="https://www.robotis.us/turtlebot-3-burger-rpi4-2gb-us/" rel="external nofollow noopener" target="_blank">Turtlebot3 Burger</a> robot (henceforth referred to as ‘the robot’) and ROS. Specifically, we will create a simple embodied agent that can navigate a grid. To say that using an LLM to navigate a grid is overkill would be a massive understatement, but this will serve as a good starting point for understanding how to create embodied agents.</p> <h2 id="system-overview">System overview</h2> <p>Our goal is to set up a system where we can ask the LLM to chart a course for the robot to reach a goal position, given the robot’s current position. The LLM will send a set of instructions to the robot, which the robot can understand and execute. The robot will then carry out these instructions to move to the goal position.</p> <p>Since the robot we are using is a differential drive robot (it can move straight and turn), we can simplify the problem by reducing the number of actions the robot can take. The robot can <code class="language-plaintext highlighter-rouge">step forward</code>, <code class="language-plaintext highlighter-rouge">turn left</code>, or <code class="language-plaintext highlighter-rouge">turn right</code>. In this experiment, we are not using any of its sensors, so the robot is essentially blind. The shortcomings and future improvements of this system will be discussed later.</p> <h2 id="preparing-the-robot">Preparing the robot</h2> <p>First, we installed <a href="https://docs.ros.org/en/humble/index.html" rel="external nofollow noopener" target="_blank">ROS Humble</a> on the Raspberry Pi 4 onboard the robot. Then, we installed the <a href="https://emanual.robotis.com/docs/en/platform/turtlebot3/overview/" rel="external nofollow noopener" target="_blank">Turtlebot3 ROS packages</a> so that we could control the robot using ROS. Finally, we created a simple ROS package that allows the robot to receive instructions from the LLM and move accordingly. There are several ways this problem can be approached, but we chose to use a simple publisher-subscriber model since this is not the focus of the project. Figure 1 shows the Turtlebot3 Burger used in the project.</p> <p align="center"> <img src="/assets/img/projects/robot-llm/turtlebot3.png" alt="the turtlebot3 burger." width="300px"> <br> <em>Figure 1: Turtlebot3 Burger.</em> </p> <p>Once this setup was complete, we were able to send an array of instructions to the robot and watch it move accordingly. For example, if the robot starts at \(\left(0,0\right)\) facing north, and we send the instructions <code class="language-plaintext highlighter-rouge">['step forward', 'turn right', 'step forward']</code>, the robot will move to \(\left(1,1\right)\) facing east. With this setup, we are basically done with the robot side of things.</p> <h2 id="using-the-llm">Using the LLM</h2> <p>In this section, I will focus solely on the approach that was successful for us. For a comprehensive discussion of all the other methods we explored, please refer to the <a href="/blog/2024/robot-llm/">accompanying blog post</a>.</p> <p>We selected the <a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-70B" rel="external nofollow noopener" target="_blank">Llama-3.1-70B</a> model for our experiment. This choice was primarily due to its open-source nature and because it is the smallest model in the Llama family that was compatible with our prompt requirements. We utilized the model in its default configuration, without any fine-tuning.</p> <p>To implement this, we created a Hugging Face space that uses Gradio to build a straightforward web interface, allowing us to input the robot’s current position and desired goal position. While we initially used this web interface for testing purposes, we eventually developed a Python script to facilitate message exchanges with the Hugging Face space. The system prompt we provided to the LLM was as follows:</p> <details> <summary> Click to see the system prompt </summary> <p> You are controlling a 2 DOF robot on a 50x50 grid. The robot can move one step in any of the four cardinal directions. The robot can perform the following actions:</p> <ul style="list-style-type: none; padding: 0; margin: 0;"> <li style="margin: 0;">- 'up': Move one unit up (increasing y coordinate by 1).</li> <li style="margin: 0;">- 'down': Move one unit down (decreasing y coordinate by 1).</li> <li style="margin: 0;">- 'left': Move one unit left (decreasing x coordinate by 1).</li> <li>- 'right': Move one unit right (increasing x coordinate by 1).</li> </ul> <p>Given a target coordinate, your task is to calculate and output the shortest sequence of commands that will move the robot from its current position to the target position.</p> Output Format: <ul style="list-style-type: none; padding: 0; margin: 0;"> <li style="margin: 0;">- Begin with the exact phrase: 'The full list is:'.</li> <li style="margin: 0;">- Provide the sequence of commands as a JSON array, with each command as a string. Commands must be exactly 'up', 'down', 'left', or 'right'.</li> <li style="margin: 0;">- All coordinates should be formatted as JSON objects with keys 'x' and 'y' and integer values. For example, the starting position should be output as {'x': 0, 'y': 0}.</li> <li style="margin: 0;">- When calling tools, ensure that all arguments use this JSON object format for coordinates, with keys 'x' and 'y'.</li> <li>- Example of correct output: If the target coordinate is {'x': 2, 'y': 3}, your response should include: 'The full list is: ["right", "right", "up", "up", "up"]'</li> </ul> <p>Please ensure that all output strictly adheres to these formats. If any output is not in the correct format, redo the task and correct the output before providing the final answer.</p> </details> <p>A detailed system prompt is essential to ensure the LLM understands exactly what is expected. It’s important to note that the system prompt instructs the LLM to provide movement directions without considering the robot’s current orientation, resulting in instructions like <code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">down</code>, <code class="language-plaintext highlighter-rouge">left</code>, or <code class="language-plaintext highlighter-rouge">right</code>. In contrast, the user prompt we used was much simpler:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">The robot is at 'start_position': {'x': 25, 'y': 24}. What is the shortest list of actions to take in sequence to get to 'target_position': {'x': 27, 'y': 25}?</code></p> </blockquote> <p>Sending both the system and user prompts to the LLM yielded the following response (Note: In this example, the robot was oriented facing south):</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Model Response: [["The robot is at 'start_position': {'x': 25, 'y': 24}. What is the shortest list of actions to take in sequence to get to 'target_position': {'x': 27, 'y': 25}?". 'The full list is: ["right", "right", "up"].']]</code></p> </blockquote> <p>A function was written to parse the response and send the instructions to the robot. Depending on the robot’s current orientation, the response was translated into commands that the robot can understand, such as (<code class="language-plaintext highlighter-rouge">step forward</code>, <code class="language-plaintext highlighter-rouge">turn left</code>, or <code class="language-plaintext highlighter-rouge">turn right</code>), and then sent to the robot. These commands were then transmitted to the robot. Since the robot continuously listens for incoming instructions, it executes them in sequence as soon as they are received.</p> <h2 id="results-limitations-and-future-work">Results, Limitations, and Future Work</h2> <p>The system performed perfectly in all the test cases we tried. The robot successfully moved to the target position in the shortest number of steps. The LLM effectively understood the user prompt and provided accurate instructions, which the robot correctly interpreted and followed.</p> <p>However, there are several limitations to this system. Since the robot operates in an open-loop manner, it essentially navigates “blind” due to a lack of sensory feedback. It only knows its initial direction because we provide it, and it can determine its final direction after executing the instructions. The robot’s movements are hardcoded, so a 90° turn isn’t always precisely 90°, leading to cumulative errors over time. Additionally, the robot can only move in discrete unit steps, so it cannot travel diagonally. This limitation means that when the goal positions are farther away and require multiple turns, the robot may end up with a significant positional error. Furthermore, the robot lacks obstacle detection and will continue moving even if something is in its path. Lastly, the LLM used is quite large, and I’m confident that a much smaller model, with some fine-tuning, could handle this task just as effectively.</p> <p>In the future, I plan to incorporate feedback from the robot’s sensors. This enhancement would allow the robot to navigate more accurately and reduce the error between its final and target positions. Additionally, I intend to fine-tune a smaller model, such as the Llama-3.1-8B, to evaluate if it can perform on par with or even better than the Llama-3.1-70B model.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Hassam Khan Wazir. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-GBHVTBHLJ9"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-GBHVTBHLJ9");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of projects that I have worked on over the years.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-teaching",title:"Teaching",description:"These are the courses I have taught over the years.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-cv",title:"CV",description:"This is my Curriculum Vitae. You can download it by clicking the button on the top right.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-nyu-hpc-setup",title:"NYU HPC Setup",description:"A guide for logging into the NYU HPC cluster and streamlining that process.",section:"Posts",handler:()=>{window.location.href="/blog/2023/nyu-hpc-setup/"}},{id:"post-the-z-domain-transfer-function-and-the-bilinear-transformation",title:"The z-domain transfer function and the Bilinear transformation.",description:"Understanding the z-domain transfer function and the bilinear transformation.",section:"Posts",handler:()=>{window.location.href="/blog/2020/imr-zdomain/"}},{id:"post-classical-controllers-p-i-d",title:"Classical Controllers (P, I, D)",description:"In this post, we will discuss the common types of controllers used in classical control systems.",section:"Posts",handler:()=>{window.location.href="/blog/2020/controls-controllers/"}},{id:"post-system-configuration-transfer-function-laplace-transform-and-system-order",title:"System Configuration, Transfer Function, Laplace Transform, and System Order.",description:"Understanding what a system is, its configuration, transfer function, Laplace transform, and system order.",section:"Posts",handler:()=>{window.location.href="/blog/2020/controls-system-config/"}},{id:"news-successfully-defended-my-ph-d-dissertation",title:"Successfully defended my Ph.D. dissertation! \ud83c\udf93",description:"",section:"News",handler:()=>{window.location.href="/news/phd-defense/"}},{id:"news-started-a-research-scientist-position-at-nyu-tandon-school-of-engineering-computer",title:'Started a Research Scientist Position at NYU Tandon School of Engineering! <img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20">',description:"",section:"News"},{id:"news-presented-a-poster-at-the-2024-ieee-engineering-medicine-and-biology-conference",title:"Presented a Poster at the 2024 IEEE Engineering Medicine and Biology Conference \ud83d\udcca...",description:"",section:"News",handler:()=>{window.location.href="/news/embc/"}},{id:"projects-wireless-earphone-based-monitoring-of-breathing-exercises",title:"Wireless Earphone-based Monitoring of Breathing Exercises",description:"This work proposes using commodity earphones for real-time breathing channel and phase detection for breathing therapy compliance monitoring.",section:"Projects",handler:()=>{window.location.href="/projects/audio-breathing/"}},{id:"projects-remote-control-of-a-dialysis-machine-using-a-human-robot-interaction",title:"Remote Control of a Dialysis Machine using a Human-Robot Interaction",description:"In this work we propose an emergency, non-invasive remote monitoring and control response system to retrofit dialysis machines with robotic manipulators for safely supporting the treatment of patients with acute kidney disease.",section:"Projects",handler:()=>{window.location.href="/projects/covid-project/"}},{id:"projects-embodied-agents-using-a-turtlebot3",title:"Embodied Agents using a Turtlebot3",description:"Understanding embodied agents with a Turtlebot3 and ROS.",section:"Projects",handler:()=>{window.location.href="/projects/robot-llm/"}},{id:"projects-using-capability-maps-and-virtual-reality-for-occupational-therapy",title:"Using Capability Maps and Virtual Reality for Occupational Therapy",description:"This work proposes using arm kinematic modeling and capability maps to allow a VR system to understand a user\u2019s physical capability and limitation.",section:"Projects",handler:()=>{window.location.href="/projects/rom-capability/"}},{id:"projects-range-of-motion-assessment-using-a-digital-voice-assistant",title:"Range of Motion Assessment using a Digital Voice Assistant",description:"This work proposes using a Digital Voice Assistant for Range of Motion measurement by utilizing 2D pose estimation techniques to estimate 3D limb pose for specific exercises.",section:"Projects",handler:()=>{window.location.href="/projects/rom-dva/"}},{id:"projects-wearable-pendant-sensor-for-therapy-compliance",title:"Wearable Pendant Sensor for Therapy Compliance",description:"A wearable pendant sensor to monitor compliance with range of motion lymphatic health exercise",section:"Projects",handler:()=>{window.location.href="/projects/rom-wps/"}},{id:"projects-a-minimalistic-puppeteering-robot",title:"A minimalistic puppeteering robot",description:"A minimalistic puppeteering robot that can convey human emotions. The robot was featured in the theatrical production Rescate at the La Moneda Cultural Center, Chile.",section:"Projects",handler:()=>{window.location.href="/projects/speaker-project/"}},{id:"projects-simulating-a-swarm-of-robots-in-gazebo",title:"Simulating a Swarm of Robots in Gazebo",description:"In this project, I simulated a swarm of robots in Gazebo. The robots were controlled using a decentralized control algorithm that allowed them to move in a formation while avoiding obstacles.",section:"Projects",handler:()=>{window.location.href="/projects/swarm-simulation/"}},{id:"projects-werable-inertial-sensors-for-exergames",title:"Werable Inertial Sensors for Exergames",description:"This work presents the design and development of wearable inertial sensors (WIS) for real-time simultaneous triplanar motion capture of the upper extremity.",section:"Projects",handler:()=>{window.location.href="/projects/wise/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%61%73%73%61%6D.%77%61%7A%69%72@%6E%79%75.%65%64%75","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0003-4930-3259","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=hBetThYAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/hassamwazir","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/hassam-wazir","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>